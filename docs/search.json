[
  {
    "objectID": "13-API-CARTO.html",
    "href": "13-API-CARTO.html",
    "title": "Acquisition",
    "section": "",
    "text": "Le but de ce chapitre est d’approfondir les méthodes de recueil de données lorsque celles-ci comportent une information géographique sur la localisation des objets étudiés. Nous allons donc charger non plus seulement des tableaux statistiques mais aussi de l’information géographique décrivant la localisation de poins, lignes ou polygones. Cela implique deux nouveautés :\nNous verrons en détail l’utilisation des données géographiques dans les chapitres ultérieurs et on se bornera ici à analyser comment recueillir cette information."
  },
  {
    "objectID": "13-API-CARTO.html#preparation-du-travail",
    "href": "13-API-CARTO.html#preparation-du-travail",
    "title": "Acquisition",
    "section": "PREPARATION DU TRAVAIL",
    "text": "PREPARATION DU TRAVAIL\n\nPackages\nOn charge les packages habituels\n\nknitr::opts_chunk$set(echo = TRUE, warning= FALSE, message = FALSE, error=FALSE)\n\n## Affichage de tableaux\nlibrary(knitr)\n\n## Requêtes web\nlibrary(httr)\nlibrary(jsonlite)\n\n## Tidyverse & co\nlibrary(dplyr, warn.conflicts = T, quietly = T)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n## Information géographique\nlibrary(geojsonsf)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\n\nOrganisation du projet\nOn crée un dossier resul à l’intérieur duquel on va imporer les données statistiques (sous-dossier stats) et des données géométriques (sous-dossier geom). On ajoute un dossier sf où on placera les ficheirs au format spatial features de R.\n\n\nObjectif Grand Paris\nOn se donne comme ojectif d’acquérir des données géométriques décrivant les communes et les iris du Grand Paris qui regroupe quatre départements 75, 92, 93, 94 et quelques communes isolées de la grande couronne dont les codes sont : “95018”,“91027”,“91479”,“91432”,“91589”,“91326”,“91687”.\nNous devons arriver à produire quatre fonds de carte :\n\npar IRIS\npar communes\npar territoires\npar département\n\nIl faudra faire en sorte que les résultats se superposent …"
  },
  {
    "objectID": "13-API-CARTO.html#importation-de-fichiers-sig",
    "href": "13-API-CARTO.html#importation-de-fichiers-sig",
    "title": "Acquisition",
    "section": "Importation de fichiers SIG",
    "text": "Importation de fichiers SIG\nBeaucoup de données géographiques ont été conçues pour l’utilisation de systèmes d’information géographiques tels que ArcGIS ou QGIS. Historiquement ces données sont le plus souvent enregistrées et échangées dans le format shapefile qui se compose de trois ou quatre fichiers pour chaque fonds de carte :\n\ncarte.shp : fichier contenant la géométrie (contour des unités)\ncarte.shx : index des unités géométriques\ncarte.prj : fichier contenant la projection de la carte\ncarte.dbf : fichier de données attributaires (statistiques)\n\nNous allons prendre à titre d’exemple l’acquisition d’un fichier des communes d’Ile de France disponible sur le site data.gouv.fr à cette adresse\nNous commeçons par télécharger le fichier avec la fonction download.file() et on stocke le résultat dans notre dossier “geom”.\n\nmyurl &lt;- \"https://www.data.gouv.fr/fr/datasets/r/5cd27d86-4859-40dc-b029-a215219eedf9\"\ndownload.file(url = myurl, destfile = \"resul/geom/idf_com.zip\")\nlist.files(\"resul/geom\")\n\n[1] \"GP_com.geojson\" \"GP_com.RDS\"     \"idf_com.zip\"   \n\n\nNous avons téléchargé un fichier au format .zip que l’on va décompresse à l’aide de la fonction unzip()\n\nzipF&lt;- \"resul/geom/idf_com.zip\"\noutDir&lt;-\"resul/geom/\"\nunzip(zipF,exdir=outDir)\nlist.files(\"resul/geom\")\n\n[1] \"communes-dile-de-france-au-01-janvier.dbf\"\n[2] \"communes-dile-de-france-au-01-janvier.prj\"\n[3] \"communes-dile-de-france-au-01-janvier.shp\"\n[4] \"communes-dile-de-france-au-01-janvier.shx\"\n[5] \"GP_com.geojson\"                           \n[6] \"GP_com.RDS\"                               \n[7] \"idf_com.zip\"                              \n\n\nOn voit bien apparaître les quatre fichiers qui définissent un shapefile et on pourrait les utiliser avec des logiciels tels que ArcGis, Qgis ou Magrit.\n\nImportantion de shapefile -&gt; R\nNous allons maintenant importer le fonds de carte dans R à l’aide du package sf (spatial features) en utilisant la fonction st_read() :\n\nmap&lt;-st_read(\"resul/geom/communes-dile-de-france-au-01-janvier.shp\")\n\nReading layer `communes-dile-de-france-au-01-janvier' from data source \n  `/Users/claudegrasland1/git/datamining2024/resul/geom/communes-dile-de-france-au-01-janvier.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1287 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1.44728 ymin: 48.12054 xmax: 3.555687 ymax: 49.23719\nGeodetic CRS:  WGS 84\n\n\nOn vérifie la classe de l’objet :\n\nclass(map)\n\n[1] \"sf\"         \"data.frame\"\n\n\nOn voit qu’il s’agit à la fois d’un data.frame et d’un objet de type sf. Regardons plus en détail avec str()\n\nstr(map)\n\nClasses 'sf' and 'data.frame':  1287 obs. of  10 variables:\n $ objectid  : num  12 14 56 57 59 73 78 86 90 94 ...\n $ shape_leng: num  16585 17790 14908 6894 10681 ...\n $ insee     : num  77472 91315 78472 77038 78672 ...\n $ nomcom    : chr  \"La Trétoire\" \"Itteville\" \"Orsonville\" \"Boissettes\" ...\n $ numdep    : num  77 91 78 77 78 95 95 95 92 77 ...\n $ fusioinsee: chr  NA NA NA NA ...\n $ nomcomto  : chr  \"Trétoire (la)\" \"Itteville\" \"Orsonville\" \"Boissettes\" ...\n $ st_areasha: num  9488102 12122472 9504776 1603038 5117345 ...\n $ st_lengths: num  16585 17790 14908 6894 10681 ...\n $ geometry  :sfc_POLYGON of length 1287; first list element: List of 1\n  ..$ : num [1:17, 1:2] 3.26 3.26 3.24 3.24 3.23 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:9] \"objectid\" \"shape_leng\" \"insee\" \"nomcom\" ...\n\n\nPar rapport à un dataframe classique on trouve une nouvelle colonne appelée “geometry” ainsi que différents attributs tels que la projection. Nous trouvons donc en un seul fichier l’ensemble des informations normalement présents dans les quatre fichiers qui composaient le shappefile.\n\n\nVisualisation de la geometrie\nPour visualiser rapidement le fonds de carte, il suffit de taper la fonction rbase plot appliquée à la colonne geometry du fichier sf :\n\nplot(map$geometry)\n\n\n\n\n\n\nExtraction du data.frame sans les données géométriques\nSi l’on veut juste travailler sur les données statistiques du tableau, on peut retirer la géométrie du fichier pour revenir à un pur objet de type data.frame à l’aide de la fonction sf st_drop_geometry() :\n\ndon&lt;-st_drop_geometry(map)\nclass(don)\n\n[1] \"data.frame\"\n\nhead(don)\n\n  objectid shape_leng insee               nomcom numdep fusioinsee\n1       12  16584.790 77472          La Trétoire     77       &lt;NA&gt;\n2       14  17789.557 91315            Itteville     91       &lt;NA&gt;\n3       56  14908.000 78472           Orsonville     78       &lt;NA&gt;\n4       57   6894.431 77038           Boissettes     77       &lt;NA&gt;\n5       59  10681.145 78672  Villennes-sur-Seine     78       &lt;NA&gt;\n6       73  18855.711 95541 Saint-Clair-sur-Epte     95       &lt;NA&gt;\n              nomcomto st_areasha st_lengths\n1        Trétoire (la)    9488102  16584.790\n2            Itteville   12122472  17789.557\n3           Orsonville    9504776  14908.000\n4           Boissettes    1603038   6894.431\n5  Villennes-sur-Seine    5117345  10681.145\n6 Saint-Clair-sur-Epte   12431151  18855.711\n\n\nOn constate que parmi les colonnes il existe à la fois une variable relative au département et une autre au code INSEE des communes. On peut donc procéder à une sélection sur ces deux critères afin d’aboutir à un fonds de carte du grand Paris.\n\n\nExtraction des communes du grand Paris\nOn utilise la condition “ou” pour filter à la fois sur les départements et les communes.\n\nGP_com &lt;- map %&gt;% filter(numdep %in% c(75, 92, 93, 94) |  \n                           insee %in% c(95018,91027,91479,91432,91589,91326,91687))\nplot(GP_com$geometry)\n\n\n\n\n\n\nCartographie rapide du résultat\nOn utilise une petite astuce pour visualiser les départements. On commence par créer une variable de type factor et on applique un plot spécial sur cette variable.\n\nGP_com$Dep&lt;-as.factor(GP_com$numdep)\nplot(GP_com[\"Dep\"])\n\n\n\n\n\n\nSauvegarde du fichier sf\nOn enregistre le fichier dans le format interne de R afin de ne pas avoir à répliquer toutes les étapes précédentes à l’aide de la fonction saveRDS(). On en profite pour nettoyer le dossier en supprimant les fichiers dont on n'a plus  besoin à l'aide de la fonctionfile.remove()pour un fichier unique ouunlink()`` pour un groupe de fichiers\n\nfile.remove(\"resul/geom/idf_com.zip\")\n\n[1] TRUE\n\nunlink(\"resul/geom/communes*\")\nsaveRDS(GP_com,\"resul/geom/GP_com.RDS\")\n\n\n\nExportation au format Geojson\nSi l’on doit travailler avec des programmeurs qui utilisent Python, le plus simple est de réaliser une exportation au format geojson à l’aide de la fonction st_write() du package sf. On rajoute l’option delete_dsn=T pour écraser une éventuelle version antérieure.\n\nst_write(GP_com, \"resul/geom/GP_com.geojson\",delete_dsn = T)\n\nDeleting source `resul/geom/GP_com.geojson' using driver `GeoJSON'\nWriting layer `GP_com' to data source \n  `resul/geom/GP_com.geojson' using driver `GeoJSON'\nWriting 150 features with 10 fields and geometry type Polygon."
  },
  {
    "objectID": "13-API-CARTO.html#importation-via-une-api",
    "href": "13-API-CARTO.html#importation-via-une-api",
    "title": "Acquisition",
    "section": "Importation via une API",
    "text": "Importation via une API\nUne solution beaucoup plus rapide consiste à importer des données géographiques en faisant appel à une API. Celles-ci renvoient en général des fichiers au format Geojson qu’il sera facile de convertir ensuite au format sf.\nA titre d’exemple, nous allons utiliser le site Opendatasoft pour importer des données concernant l’indice de défavorisation sociale à l’échelle des IRIS dans la commune de Fontenay-sous-Bois\n\n\n\nIndice de déprivation sociale\n\n\n\nIdentification du lien de téléchargement\nOn procède à une sélection des IRIS de la région Ile-de-France puis on copie le lien permettant de récupérer les données au format shapefile\n\n\n\nlien de télécargement\n\n\n\n\nRécupération du fichier au format sf\n\nmyurl &lt;- \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/indice-de-defavorisation-sociale-fdep-par-iris/exports/geojson?lang=fr&refine=c_nom_com%3A%22FONTENAY-SOUS-BOIS%22&timezone=Europe%2FBerlin\"\nmap &lt;- st_read(myurl)\n\nReading layer `OGRGeoJSON' from data source \n  `https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/indice-de-defavorisation-sociale-fdep-par-iris/exports/geojson?lang=fr&refine=c_nom_com%3A%22FONTENAY-SOUS-BOIS%22&timezone=Europe%2FBerlin' \n  using driver `GeoJSON'\nSimple feature collection with 21 features and 14 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2.447158 ymin: 48.8391 xmax: 2.500071 ymax: 48.86146\nGeodetic CRS:  WGS 84\n\n\n\n\nVisualisation du fonds de carte\n\nplot(map$geometry)\n\n\n\n\n\n\nCartographie rapide d’indicateurs\n\nplot(map[\"t1_rev_med\"], main=\"Revenu médian\")\n\n\n\n\n\nplot(map[\"t1_txbac09\"], main=\"Diplômes du supérieur\")\n\n\n\n\n\nplot(map[\"t1_txchom0\"], main=\"Taux de chômage\")\n\n\n\n\n\nplot(map[\"t1_txouvr0\"], main=\"Part des ouvriers\")\n\n\n\n\n\n\nExercice\n\nConstruire un indicateur synthétique combinant les quatre variables\nCartographier cet indicateur\nConstruire une fonction applicable à une commune quelconque."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Mining 2024",
    "section": "",
    "text": "Ce document est la quatrième édition d’un cours de Data Mining dispensé aux étudiants de deuxième année de l’ option Data Mining du master MECI conjointement avec Camille Signoretto.\nIl est basé sur :\n\nR.Version()[[\"version.string\"]]\n\n[1] \"R version 4.3.1 (2023-06-16)\"\n\n\nCe document est régulièrement corrigé et mis à jour. La version de référence est disponible en ligne à l’adresse :\n\nhttps://claudegrasland.github.io/datamining2024/.\n\nPour toute suggestion ou correction, il est possible de me contacter par mail"
  },
  {
    "objectID": "index.html#à-propos-de-ce-document",
    "href": "index.html#à-propos-de-ce-document",
    "title": "Data Mining 2024",
    "section": "",
    "text": "Ce document est la quatrième édition d’un cours de Data Mining dispensé aux étudiants de deuxième année de l’ option Data Mining du master MECI conjointement avec Camille Signoretto.\nIl est basé sur :\n\nR.Version()[[\"version.string\"]]\n\n[1] \"R version 4.3.1 (2023-06-16)\"\n\n\nCe document est régulièrement corrigé et mis à jour. La version de référence est disponible en ligne à l’adresse :\n\nhttps://claudegrasland.github.io/datamining2024/.\n\nPour toute suggestion ou correction, il est possible de me contacter par mail"
  },
  {
    "objectID": "index.html#prérequis",
    "href": "index.html#prérequis",
    "title": "Data Mining 2024",
    "section": "Prérequis",
    "text": "Prérequis\nLe seul prérequis pour suivre ce document est d’avoir installé R et RStudio sur votre ordinateur. Il s’agit de deux logiciels libres, gratuits, téléchargeables en ligne et fonctionnant sous PC, Mac et Linux.\nPour installer R, il suffit de se rendre sur une des pages suivantes 1 :\n\nInstaller R sous Windows\nInstaller R sous Mac\n\nPour installer RStudio, rendez-vous sur la page suivante et téléchargez la version adaptée à votre système :\n\nhttps://www.rstudio.com/products/rstudio/download/#download"
  },
  {
    "objectID": "index.html#remerciements",
    "href": "index.html#remerciements",
    "title": "Data Mining 2024",
    "section": "Remerciements",
    "text": "Remerciements\nCe document a bénéficié de la relecture et des suggestions … des étudiants qui en ont été les cobayes des premières versions."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Data Mining 2024",
    "section": "Licence",
    "text": "Licence\nCe document est mis à disposition selon les termes de la Licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International.\n\n\n\nLicence Creative Commons"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Data Mining 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSous Linux, utilisez votre gestionnaire de packages habituel.↩︎"
  },
  {
    "objectID": "21-CARTO-ParisPC.html",
    "href": "21-CARTO-ParisPC.html",
    "title": "Jointures",
    "section": "",
    "text": "On charge les packages suivants\n\nknitr::opts_chunk$set(echo = TRUE, warning= FALSE, message = FALSE, error=FALSE)\n\n## Affichage de tableaux\nlibrary(knitr)\n\n## Requêtes web\nlibrary(jsonlite)\n\n## Tidyverse & co\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Data.table (pour sa fonction d'importation fread)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n## Information géographique\nlibrary(geojsonsf)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n## Cartographie thématique\nlibrary(mapsf)\n\n\n\n\nOn constitue un fichier des individus localisés dans les communes de Paris et Petite Couronne au RP de 2019 en reprenant le programme décrit dans le Cours n°1 de Camille Signoretto.\nLe programme ci-dessous comporte la mention “eval=FALSE” car il ne dout être executé qu’une seule fois.\n\n## Récupération des fichiers INSEE zippé\n## On utilise pour cela un dossier \"tmp\" \ndownload.file(url=\"https://www.insee.fr/fr/statistiques/fichier/6544333/RP2019_INDCVIZA_csv.zip\",\n              destfile = \"tmp/RP2019_INDCVIZA_csv.zip\")\nunzip(\"tmp/RP2019_INDCVIZA_csv.zip\", exdir = \"tmp\")\n\n## Lecture du fichier individu avec fread\nlibrary(data.table)\nRP &lt;- fread(\"tmp/FD_INDCVIZA_2019.csv\", stringsAsFactors=TRUE)\nRP &lt;- as.data.frame(RP)\n## Selection Paris PC \nRP &lt;- RP %&gt;% filter(DEPT %in% c(75, 92, 93, 94))\nsaveRDS(RP, \"data/RP/RP_final.RDS\")\n\n## Lecture du fichier de métadonnées\nmeta &lt;- read.csv(file = 'tmp/Varmod_INDCVI_2019.csv',\n                 sep = \";\",\n                 encoding = \"UTF-8\",\n                 stringsAsFactors = TRUE)\n\n## Sauvegarde des deux fichiers\nsaveRDS(meta, \"data/RP/meta.RDS\")\n\n## nettoyage du dossier  tmp\nunlink(\"tmp/*\")\n\n\n\n\nOn va maintenant acquérir le fichier des unités géographiques les plus petites (IRIS) pour la zone Paris + Petite Couronne. On se servira de ce fonds de carte des IRIS pour générer ensuite ceux des unités géographiques de niveau supérieur : communes, territoires, départements …\nComme les IRIS changent au cours du temps, il faut choisir le bon “millésime” pour que la correspondance soit possible avec les données individuelles du recensement. On utilise un lien de téléchargement depuis la base des iris millésimé accessible sur public.opendatasoft\nComme précédemment, ce programme est à executer une seule fois d’où la mention eval=FALSE dans l’en-tête du chunk.\n\n## Lien de téléchargement IDF 2019 au forma geojson\nmyurl &lt;-\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-france-iris-millesime/exports/geojson?lang=fr&refine=year%3A%222019%22&refine=reg_name%3A%22%C3%8Ele-de-France%22&facet=facet(name%3D%22reg_name%22%2C%20disjunctive%3Dtrue)&timezone=Europe%2FBerlin\"\n\n## téléchargement et conversion au format sf\ngeo&lt;-geojson_sf(myurl)\ngeo&lt;-geo %&gt;% select(iris_type,\n                    iris_code, \n                    iris_name,\n                    com_code= com_arm_code,\n                    com_name = com_arm_name,\n                    dep_code,\n                    dep_name,\n                    geometry)\n\n## Nettoyage des chaînes de caractère\nclean_char &lt;- function(x) {\n  y&lt;-gsub('\\\\[\\\"','',x)\n  y&lt;-gsub('\\\"\\\\]','',y)\n  return(y)\n}\ngeo &lt;- geo %&gt;% mutate(iris_code = clean_char(iris_code),\n                      iris_name = clean_char(iris_name),\n                      com_code = clean_char(com_code),\n                      com_name = clean_char(com_name),\n                      dep_code = clean_char(dep_code),\n                      dep_name = clean_char(dep_name),\n                      )\n\n\n\n## Selection Paris PC\ngeo&lt;-geo %&gt;% filter(dep_code %in% c(\"75\",\"92\",\"93\",\"94\"))\n#plot(geo[\"iris_type\"])\n\n## Sauvegarde\n\nsaveRDS(geo,\"data/RP/map_iris.RDS\" )\n\n\n## Carto rapide\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nplot(map_iris[\"dep_code\"], main = \"IRIS\")\n\n\n\n\n\n\n\nOn agrège par le nom et le code de la commune et on conserve le nom et le code du département.\n\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nmap_com &lt;- map_iris %&gt;% group_by(com_code, com_name) %&gt;% \n                  summarise(dep_code = max(dep_code),\n                            dep_name = max(dep_name))\nplot(map_com[\"dep_code\"], main = \"Communes\")\n\n\n\nsaveRDS(map_com, \"DATA/RP/map_com.RDS\")\n\n\n\n\nOn agrège simplement par le nom et le code du département.\n\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nmap_dep &lt;- map_iris %&gt;% group_by(dep_code, dep_name) %&gt;% \n                  summarise()\nplot(map_dep[\"dep_code\"], main = \"Départements\")\n\n\n\nsaveRDS(map_dep, \"DATA/RP/map_dep.RDS\")\n\n\n\n\nNous allons maintenant examiner comment agréger les données individuelles de l’INSEE par iris, commune ou département et effectuer une jointure avec les fonds de cartes que nous avons préparé. On va s’appuyer pour cela sur le cours de datamining n°2 de Camille Signoretto.\nOn commence par recharger la fonction somme()que nous avions créé :\n\nsomme &lt;- function(data, var_gpe, nom_var){\n  som &lt;- data %&gt;% \n    group_by({{var_gpe}}) %&gt;% \n    count({{nom_var}}, wt=IPONDI) %&gt;% \n    mutate(n=round(n)) %&gt;% \n    pivot_wider(names_from = {{nom_var}}, values_from = n)\n  \n  return(som)\n}\n\nNous l’utilisons pour créer un tableau des individus par CSP simplifiées en 5 catégories et par IRIS :\n\n# Chargement du tableau\nindiv &lt;- readRDS(\"data/RP/RP_final.RDS\")\n\n# Création de CSP simplifiées\nindiv &lt;- indiv %&gt;% \n  mutate(TACT5=case_when(TACT == \"11\" ~ \"EMP\",\n                             TACT == \"12\" ~ \"CHO\",\n                             TACT == \"22\" ~ \"ETU\",\n                             TACT == \"21\" ~ \"RET\",\n                             TRUE ~ \"DIV\"))\n\n# Agrégation par IRIS  \niris_csp &lt;- somme(data = indiv,\n                 var_gpe = IRIS,\n                 nom_var = TACT5)\n\n\n\n\nOn procède à la jointure des deux fichiers iris en utilisant le code des iris.\n\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nmap_iris_csp&lt;- left_join(map_iris, iris_csp,by = c(\"iris_code\"=\"IRIS\"))\nsaveRDS(map_iris_csp, \"data/RP/map_iris_CSP.RDS\")\n\nOn examine quels IRIS ne sont pas renseignés en croisant cette information avec le type d’IRIS.\n\nmap_iris_csp$missing&lt;-case_when(is.na(map_iris_csp$CHO)  ~ \"Manquant\",\n                                TRUE ~ \"OK\"\n                               )\nplot(map_iris_csp[\"missing\"], main = \"IRIS sans données\")\n\n\n\ntab&lt;-table(map_iris_csp$missing,map_iris_csp$iris_type)\naddmargins(tab)\n\n          \n           commune iris d'activité iris d'habitat iris divers  Sum\n  Manquant       7              44              5          40   96\n  OK             0              74           2572           7 2653\n  Sum            7             118           2577          47 2749\n\n\nOn constate qu’il manque des données pour 96 IRIS sur 2749. Il s’agit dans la plupart des cas d’iris correspondant à des zones industrielles ou des forêts dont le nombre d’habitant est trop faible pour que les données soient mises à disposition au niveau individuel. Cela concerne également 7 communes de petites tailles et 5 iris d’habitat.\n\n\n\nOn reprend les programmes précédents par commune\n\n# Chargement du tableau\nindiv &lt;- readRDS(\"data/RP/RP_final.RDS\")\n\n# Création de CSP simplifiées\nindiv &lt;- indiv %&gt;% \n  mutate(TACT5=case_when(TACT == \"11\" ~ \"EMP\",\n                             TACT == \"12\" ~ \"CHO\",\n                             TACT == \"22\" ~ \"ETU\",\n                             TACT == \"21\" ~ \"RET\",\n                             TRUE ~ \"DIV\")) \n\n# Extraction du code communal \nindiv&lt;- indiv %&gt;%   mutate(com_code = substr(IRIS,1,5))\n\n# Agrégation par IRIS  \ncom_csp &lt;- somme(data = indiv,\n                 var_gpe = com_code,\n                 nom_var = TACT5)\n\n# Chargement du fonds de carte communal\nmap_com &lt;- readRDS(\"data/RP/map_com.RDS\")\n\n# Jointure\nmap_com_csp &lt;- left_join(map_com, com_csp)\n\n# Sauvegarde\nsaveRDS(map_com_csp, \"data/RP/map_com_csp.RDS\")\n\n# Analyse des valeurs manquantes\nmap_com_csp$missing&lt;-case_when(is.na(map_com_csp$CHO)  ~ \"Manquant\",\n                                TRUE ~ \"OK\"\n                               )\nplot(map_com_csp[\"missing\"], main= \"Communes sans données\")\n\n\n\n\nOn retrouve les 7 communes manquantes pour lesquelles l’INSEE ne fournit pas les données dans le fichier détail des individus."
  },
  {
    "objectID": "21-CARTO-ParisPC.html#preparation-du-travail",
    "href": "21-CARTO-ParisPC.html#preparation-du-travail",
    "title": "Jointures",
    "section": "",
    "text": "On charge les packages suivants\n\nknitr::opts_chunk$set(echo = TRUE, warning= FALSE, message = FALSE, error=FALSE)\n\n## Affichage de tableaux\nlibrary(knitr)\n\n## Requêtes web\nlibrary(jsonlite)\n\n## Tidyverse & co\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Data.table (pour sa fonction d'importation fread)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n## Information géographique\nlibrary(geojsonsf)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n## Cartographie thématique\nlibrary(mapsf)\n\n\n\n\nOn constitue un fichier des individus localisés dans les communes de Paris et Petite Couronne au RP de 2019 en reprenant le programme décrit dans le Cours n°1 de Camille Signoretto.\nLe programme ci-dessous comporte la mention “eval=FALSE” car il ne dout être executé qu’une seule fois.\n\n## Récupération des fichiers INSEE zippé\n## On utilise pour cela un dossier \"tmp\" \ndownload.file(url=\"https://www.insee.fr/fr/statistiques/fichier/6544333/RP2019_INDCVIZA_csv.zip\",\n              destfile = \"tmp/RP2019_INDCVIZA_csv.zip\")\nunzip(\"tmp/RP2019_INDCVIZA_csv.zip\", exdir = \"tmp\")\n\n## Lecture du fichier individu avec fread\nlibrary(data.table)\nRP &lt;- fread(\"tmp/FD_INDCVIZA_2019.csv\", stringsAsFactors=TRUE)\nRP &lt;- as.data.frame(RP)\n## Selection Paris PC \nRP &lt;- RP %&gt;% filter(DEPT %in% c(75, 92, 93, 94))\nsaveRDS(RP, \"data/RP/RP_final.RDS\")\n\n## Lecture du fichier de métadonnées\nmeta &lt;- read.csv(file = 'tmp/Varmod_INDCVI_2019.csv',\n                 sep = \";\",\n                 encoding = \"UTF-8\",\n                 stringsAsFactors = TRUE)\n\n## Sauvegarde des deux fichiers\nsaveRDS(meta, \"data/RP/meta.RDS\")\n\n## nettoyage du dossier  tmp\nunlink(\"tmp/*\")\n\n\n\n\nOn va maintenant acquérir le fichier des unités géographiques les plus petites (IRIS) pour la zone Paris + Petite Couronne. On se servira de ce fonds de carte des IRIS pour générer ensuite ceux des unités géographiques de niveau supérieur : communes, territoires, départements …\nComme les IRIS changent au cours du temps, il faut choisir le bon “millésime” pour que la correspondance soit possible avec les données individuelles du recensement. On utilise un lien de téléchargement depuis la base des iris millésimé accessible sur public.opendatasoft\nComme précédemment, ce programme est à executer une seule fois d’où la mention eval=FALSE dans l’en-tête du chunk.\n\n## Lien de téléchargement IDF 2019 au forma geojson\nmyurl &lt;-\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-france-iris-millesime/exports/geojson?lang=fr&refine=year%3A%222019%22&refine=reg_name%3A%22%C3%8Ele-de-France%22&facet=facet(name%3D%22reg_name%22%2C%20disjunctive%3Dtrue)&timezone=Europe%2FBerlin\"\n\n## téléchargement et conversion au format sf\ngeo&lt;-geojson_sf(myurl)\ngeo&lt;-geo %&gt;% select(iris_type,\n                    iris_code, \n                    iris_name,\n                    com_code= com_arm_code,\n                    com_name = com_arm_name,\n                    dep_code,\n                    dep_name,\n                    geometry)\n\n## Nettoyage des chaînes de caractère\nclean_char &lt;- function(x) {\n  y&lt;-gsub('\\\\[\\\"','',x)\n  y&lt;-gsub('\\\"\\\\]','',y)\n  return(y)\n}\ngeo &lt;- geo %&gt;% mutate(iris_code = clean_char(iris_code),\n                      iris_name = clean_char(iris_name),\n                      com_code = clean_char(com_code),\n                      com_name = clean_char(com_name),\n                      dep_code = clean_char(dep_code),\n                      dep_name = clean_char(dep_name),\n                      )\n\n\n\n## Selection Paris PC\ngeo&lt;-geo %&gt;% filter(dep_code %in% c(\"75\",\"92\",\"93\",\"94\"))\n#plot(geo[\"iris_type\"])\n\n## Sauvegarde\n\nsaveRDS(geo,\"data/RP/map_iris.RDS\" )\n\n\n## Carto rapide\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nplot(map_iris[\"dep_code\"], main = \"IRIS\")\n\n\n\n\n\n\n\nOn agrège par le nom et le code de la commune et on conserve le nom et le code du département.\n\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nmap_com &lt;- map_iris %&gt;% group_by(com_code, com_name) %&gt;% \n                  summarise(dep_code = max(dep_code),\n                            dep_name = max(dep_name))\nplot(map_com[\"dep_code\"], main = \"Communes\")\n\n\n\nsaveRDS(map_com, \"DATA/RP/map_com.RDS\")\n\n\n\n\nOn agrège simplement par le nom et le code du département.\n\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nmap_dep &lt;- map_iris %&gt;% group_by(dep_code, dep_name) %&gt;% \n                  summarise()\nplot(map_dep[\"dep_code\"], main = \"Départements\")\n\n\n\nsaveRDS(map_dep, \"DATA/RP/map_dep.RDS\")\n\n\n\n\nNous allons maintenant examiner comment agréger les données individuelles de l’INSEE par iris, commune ou département et effectuer une jointure avec les fonds de cartes que nous avons préparé. On va s’appuyer pour cela sur le cours de datamining n°2 de Camille Signoretto.\nOn commence par recharger la fonction somme()que nous avions créé :\n\nsomme &lt;- function(data, var_gpe, nom_var){\n  som &lt;- data %&gt;% \n    group_by({{var_gpe}}) %&gt;% \n    count({{nom_var}}, wt=IPONDI) %&gt;% \n    mutate(n=round(n)) %&gt;% \n    pivot_wider(names_from = {{nom_var}}, values_from = n)\n  \n  return(som)\n}\n\nNous l’utilisons pour créer un tableau des individus par CSP simplifiées en 5 catégories et par IRIS :\n\n# Chargement du tableau\nindiv &lt;- readRDS(\"data/RP/RP_final.RDS\")\n\n# Création de CSP simplifiées\nindiv &lt;- indiv %&gt;% \n  mutate(TACT5=case_when(TACT == \"11\" ~ \"EMP\",\n                             TACT == \"12\" ~ \"CHO\",\n                             TACT == \"22\" ~ \"ETU\",\n                             TACT == \"21\" ~ \"RET\",\n                             TRUE ~ \"DIV\"))\n\n# Agrégation par IRIS  \niris_csp &lt;- somme(data = indiv,\n                 var_gpe = IRIS,\n                 nom_var = TACT5)\n\n\n\n\nOn procède à la jointure des deux fichiers iris en utilisant le code des iris.\n\nmap_iris &lt;- readRDS(\"data/RP/map_iris.RDS\")\nmap_iris_csp&lt;- left_join(map_iris, iris_csp,by = c(\"iris_code\"=\"IRIS\"))\nsaveRDS(map_iris_csp, \"data/RP/map_iris_CSP.RDS\")\n\nOn examine quels IRIS ne sont pas renseignés en croisant cette information avec le type d’IRIS.\n\nmap_iris_csp$missing&lt;-case_when(is.na(map_iris_csp$CHO)  ~ \"Manquant\",\n                                TRUE ~ \"OK\"\n                               )\nplot(map_iris_csp[\"missing\"], main = \"IRIS sans données\")\n\n\n\ntab&lt;-table(map_iris_csp$missing,map_iris_csp$iris_type)\naddmargins(tab)\n\n          \n           commune iris d'activité iris d'habitat iris divers  Sum\n  Manquant       7              44              5          40   96\n  OK             0              74           2572           7 2653\n  Sum            7             118           2577          47 2749\n\n\nOn constate qu’il manque des données pour 96 IRIS sur 2749. Il s’agit dans la plupart des cas d’iris correspondant à des zones industrielles ou des forêts dont le nombre d’habitant est trop faible pour que les données soient mises à disposition au niveau individuel. Cela concerne également 7 communes de petites tailles et 5 iris d’habitat.\n\n\n\nOn reprend les programmes précédents par commune\n\n# Chargement du tableau\nindiv &lt;- readRDS(\"data/RP/RP_final.RDS\")\n\n# Création de CSP simplifiées\nindiv &lt;- indiv %&gt;% \n  mutate(TACT5=case_when(TACT == \"11\" ~ \"EMP\",\n                             TACT == \"12\" ~ \"CHO\",\n                             TACT == \"22\" ~ \"ETU\",\n                             TACT == \"21\" ~ \"RET\",\n                             TRUE ~ \"DIV\")) \n\n# Extraction du code communal \nindiv&lt;- indiv %&gt;%   mutate(com_code = substr(IRIS,1,5))\n\n# Agrégation par IRIS  \ncom_csp &lt;- somme(data = indiv,\n                 var_gpe = com_code,\n                 nom_var = TACT5)\n\n# Chargement du fonds de carte communal\nmap_com &lt;- readRDS(\"data/RP/map_com.RDS\")\n\n# Jointure\nmap_com_csp &lt;- left_join(map_com, com_csp)\n\n# Sauvegarde\nsaveRDS(map_com_csp, \"data/RP/map_com_csp.RDS\")\n\n# Analyse des valeurs manquantes\nmap_com_csp$missing&lt;-case_when(is.na(map_com_csp$CHO)  ~ \"Manquant\",\n                                TRUE ~ \"OK\"\n                               )\nplot(map_com_csp[\"missing\"], main= \"Communes sans données\")\n\n\n\n\nOn retrouve les 7 communes manquantes pour lesquelles l’INSEE ne fournit pas les données dans le fichier détail des individus."
  },
  {
    "objectID": "12-API-exo.html",
    "href": "12-API-exo.html",
    "title": "API-Exo",
    "section": "",
    "text": "Nous proposons une série d’exercices d’application du cours du chapitre précédent en allant des applications les plus simples au plux complexes. Les exercices portent tous sur la base de donnée des demandes de valeurs foncières géoloalisées que l’on peut trouver sur le site public.opendatasoft"
  },
  {
    "objectID": "12-API-exo.html#exercice-1-récupération-et-analyse-dun-tableau-unique",
    "href": "12-API-exo.html#exercice-1-récupération-et-analyse-dun-tableau-unique",
    "title": "API-Exo",
    "section": "Exercice 1 : Récupération et analyse d’un tableau unique",
    "text": "Exercice 1 : Récupération et analyse d’un tableau unique\n\nProblème\nEssayez de récupérer à l’aide d’une API les informations sur l’ensemble des ventes immobilières de maisons de la commune de Montcuq-en-Quercy-Blanc (code INSEE = 46201) au cours de l’année 2020. Vous devez ensuite\n\nAfficher les premières lignes du tableau des ventes de maisons à Moncuq en en 2020\nCalculer le nombre de ventes et leur prix moyen au m2\nRéaliser un histogramme du prix moyen de ces ventes sur lequel figureront le nombre de ventes et le prix moyen.\n\n\n\nSolution\nVous devez obtenir les résultats suivants :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nCommune\nCode\nSurf_hab\nSurf_ter\nPrix\nPrix_m2\n\n\n\n\n2020-01-06\nMontcuq-en-Quercy-Blanc\n46201\n280\n450\n270000\n964\n\n\n2020-01-31\nMontcuq-en-Quercy-Blanc\n46201\n60\n1258\n60000\n1000\n\n\n2020-02-07\nMontcuq-en-Quercy-Blanc\n46201\n27\n1275\n187000\n6926\n\n\n2020-02-07\nMontcuq-en-Quercy-Blanc\n46201\n79\n1275\n187000\n2367\n\n\n2020-02-20\nMontcuq-en-Quercy-Blanc\n46201\n40\nNA\n18000\n450\n\n\n2020-03-16\nMontcuq-en-Quercy-Blanc\n46201\n50\n800\n121000\n2420\n\n\n\n\n\n[1] \"Il ya eu 39 ventes au prix moyen de 1910 €/m2\""
  },
  {
    "objectID": "12-API-exo.html#exercice-2-tableau-de-bord-dune-commune",
    "href": "12-API-exo.html#exercice-2-tableau-de-bord-dune-commune",
    "title": "API-Exo",
    "section": "Exercice 2 : Tableau de bord d’une commune",
    "text": "Exercice 2 : Tableau de bord d’une commune\n\nProblème\nEssayez de récupérer à l’aide d’une API les informations sur l’ensemble des ventes immobilières de maisons ou d’appartement de la commune de Sucy-en-Brie (Code INSEE = 94071)\n\nSimplifiez le tableau pour ne garder que les variables suivantes\n\n\ndate : date de la transaction\ncode : code INSEE de la commune\nbien : type de bien (maison ou appartement)\nnom : nom de la commune\nprix : prix de vente total\nsurf : surface habitable\nprixm2 : prix au m2\n\n\nNettoyer le tableau en retirant les transactions dont le prix au m2 est supérieur à 10000€\nCréez un tableau montrant l’évolution par année des prix médian au m2 des maisons et des appartements.\nCréez un graphique montrant l’évolution mensuelle des prix au m2 des maisons et des appartements.\n\n\n\nSolution\nVous devez obtenir les résultats suivants :\n\n\n\n\n\ndate\ncode\nnom\nbien\nprix\nsurf\nprix_m2\n\n\n\n\n2014-01-10\n94071\nSucy-en-Brie\nAppartement\n212000\n62\n3419\n\n\n2014-01-16\n94071\nSucy-en-Brie\nMaison\n640000\n145\n4414\n\n\n2014-01-16\n94071\nSucy-en-Brie\nMaison\n640000\n145\n4414\n\n\n2014-01-17\n94071\nSucy-en-Brie\nAppartement\n115480\n45\n2566\n\n\n2014-01-21\n94071\nSucy-en-Brie\nMaison\n390000\n132\n2955\n\n\n2014-01-23\n94071\nSucy-en-Brie\nMaison\n690300\n140\n4931\n\n\n\n\n\n\nPrix médian de vente des maisons et appartement (en €/m2)\n\n\nAnnée\nVentes d’appartements\nVentes de maisons\n\n\n\n\n2014\n3200\n3666\n\n\n2015\n2889\n3541\n\n\n2016\n3069\n3636\n\n\n2017\n3249\n3783\n\n\n2018\n3455\n3753\n\n\n2019\n3537\n4097"
  },
  {
    "objectID": "12-API-exo.html#exercice-3-automatisation",
    "href": "12-API-exo.html#exercice-3-automatisation",
    "title": "API-Exo",
    "section": "Exercice 3 : Automatisation",
    "text": "Exercice 3 : Automatisation\nEcrivez le progamme de l’exercice 2 sous la forme d’une fonction prenant en entrée le code INSEE d’une commune quelconque."
  },
  {
    "objectID": "10-API-INTRO.html",
    "href": "10-API-INTRO.html",
    "title": "Introduction aux API",
    "section": "",
    "text": "library(knitr,warn.conflicts = T,quietly = T)\nlibrary(dplyr, warn.conflicts = T,quietly = T)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "10-API-INTRO.html#introduction",
    "href": "10-API-INTRO.html#introduction",
    "title": "Introduction aux API",
    "section": "Introduction",
    "text": "Introduction\n\nDéfinition\nOn peut partir de la définition suivante:\n\nEn informatique, API est l’acronyme d’Application Programming Interface, que l’on traduit en français par interface de programmation applicative ou interface de programmation d’application. L’API peut être résumée à une solution informatique qui permet à des applications de communiquer entre elles et de s’échanger mutuellement des services ou des données. Il s’agit en réalité d’un ensemble de fonctions qui facilitent, via un langage de programmation, l’accès aux services d’une application. (Source : Journal du Net)\n\n\n\nFonctions\nUne API peut remplir des fonctions très diverses :\n\nDans le domaine d’internet, l’API permet aux développeurs de pouvoir utiliser un programme sans avoir à se soucier du fonctionnement complexe d’une application. Les API peuvent par exemple être utilisées pour déclencher des campagnes publicitaires d’e-mailing de façon automatique sans avoir à passer par la compréhension d’une telle application (c’est le cas avec l’API AdWords de Google, par exemple). On les retrouve aujourd’hui dans de nombreux logiciels, en particulier dans les systèmes d’exploitation, les serveurs d’applications, dans le monde du graphisme (OpenGL), dans les applications SaaS (Office 365, G Suite, Salesforce…), les bases de données, l’open data, etc.(Source : Journal du Net)\n\n\n\nProtocoles\nD’une manière générale, les API supposent un échange d’informations entre un client et un serveur.\n\nCes échanges d’informations suivent un protocole c’est-à-dire un ensemble de règles. Il existe deux grands protocoles de communication sur lesquels s’adossent les API : Simple Object Access Protocol (SOAP) et Representational State Transfer (REST). Le second s’est désormais largement imposé face au premier car il est plus flexible. Il a donné naissance aux API dites REST ou RESTful (Source : Journal du Net)\n\n\n\nAPI et Data Science\nLe métier de data analyst implique presque nécessairement l’emploi d’API. Les langages de programmation R ou Python ont donc l’un comme l’autre mis au point des packages pour faciliter l’envoi de requêtes sur des serveurs dotés d’API.\n\n«API» est un terme général désignant le lieu où un programme informatique interagit avec un autre ou avec lui-même. Dans ce didacticiel, nous travaillerons spécifiquement avec des API Web, où deux ordinateurs différents - un client et un serveur - interagiront l’un avec l’autre pour demander et fournir des données, respectivement.\n\n\nLes API offrent aux scientifiques des données un moyen raffiné de demander des données propres et organisées à partir d’un site Web. Lorsqu’un site Web comme Facebook met en place une API, il met essentiellement en place un ordinateur qui attend les demandes de données.\n\n\nUne fois que cet ordinateur reçoit une demande de données, il effectuera son propre traitement des données et les enverra à l’ordinateur qui l’a demandé. De notre point de vue en tant que demandeur, nous devrons écrire du code dans R qui crée la demande et indique à l’ordinateur exécutant l’API ce dont nous avons besoin. Cet ordinateur lira ensuite notre code, traitera la requête et renverra des données bien formatées qui peuvent être facilement analysées par les bibliothèques R existantes.\n\n\nPourquoi est-ce précieux? Comparez l’approche API au scraping Web pur. Lorsqu’un programmeur gratte une page Web, il reçoit les données dans un morceau de HTML désordonné. Bien qu’il existe certainement des bibliothèques qui facilitent l’analyse du texte HTML, ce sont toutes des étapes de nettoyage qui doivent être prises avant même de mettre la main sur les données que nous voulons!\n\n\nSouvent, nous pouvons immédiatement utiliser les données que nous obtenons d’une API, ce qui nous fait gagner du temps et de la frustration.\n\nSource : Traduction française d’un billet de Pascual C., 2020"
  },
  {
    "objectID": "10-API-INTRO.html#lapi-de-la-nasa",
    "href": "10-API-INTRO.html#lapi-de-la-nasa",
    "title": "Introduction aux API",
    "section": "l’API de la NASA",
    "text": "l’API de la NASA\nA titre d’exemple, C. Pascual propose de travailler avec l’API Open Notify, qui donne accès à des données sur divers projets de la NASA. À l’aide de l’API Open Notify, nous pouvons notamment en savoir plus sur l’emplacement de la Station spatiale internationale et sur le nombre de personnes actuellement dans l’espace.\n\nInstaller les packages jsonlite et httr\nPour travailler avec des API dans R, nous devons intégrer certaines bibliothèques (library). Ces bibliothèques prennent toutes les complexités d’une requête d’API et les enveloppent dans des fonctions que nous pouvons utiliser dans des lignes de code uniques. Les bibliothèques R que nous utiliserons sont httr et jsonlite. Elles remplissent des rôles différents dans notre introduction des API, mais les deux sont essentiels.Si vous ne disposez pas de ces bibliothèques dans votre console R ou RStudio, vous devez d’abord les télécharger.\n\nlibrary(httr)\nlibrary(jsonlite)\n\n\n\nFormulation d’une requête GET()\nUne requête adressé à une API va suivre le schéma suivant :\n\nknitr::include_graphics(\"img/API_GET.png\",)\n\n\n\n\nIl existe plusieurs types de requêtes que l’on peut adresser à un serveur API. Pour nos besoins, nous allons simplement demander des données, ce qui correspond à une demande GET. Les autres types de requêtes sont POST et PUT, mais nous n’avons pas à nous en préoccuper dans l’immédiat\nAfin de créer une requête GET, nous devons utiliser la fonction GET() de la bibliothèque httr. La fonction GET() nécessite une URL, qui spécifie l’adresse du serveur auquel la demande doit être envoyée.\nNotre programme télécharge les données disponibles à l’adresse du serveur et les stocke dans un objet auquel on peut donner le nom que l’on souhaite, par exemple ovni dans la mesure où le résultat est de prime abord assez mystérieux…\n\novni &lt;- GET(\"http://api.open-notify.org/astros.json\")\nclass(ovni)\n\n[1] \"response\"\n\n\nOn sait que la classe de l’objet est de type response ce qui ne nous avance pas beaucoup.\nToutefois, si on demande à l’objet de s’afficher il nous apporte quatre renseignements utiles\n\novni\n\nResponse [http://api.open-notify.org/astros.json]\n  Date: 2024-03-09 13:57\n  Status: 200\n  Content-Type: application/json\n  Size: 360 B\n\n\n\nDate : le moment exact du téléchargement, très utile pour suivre les mises à jour\nStatus : le code informatique de résultat de la requête. La valeur 200 indique un succès alors que les autres valeurs signaleront un problème.\nContent-Type : le type d’information recueillie. Ici, une application au format json\nSize : la taille du fichier résultant du transfert.\n\nOn poursuit notre enquête en tapant la commande str() qui permet d’avoir plus de détail sur le contenu de l’objet.\n\nstr(ovni)\n\nList of 10\n $ url        : chr \"http://api.open-notify.org/astros.json\"\n $ status_code: int 200\n $ headers    :List of 6\n  ..$ server                     : chr \"nginx/1.10.3\"\n  ..$ date                       : chr \"Sat, 09 Mar 2024 13:57:20 GMT\"\n  ..$ content-type               : chr \"application/json\"\n  ..$ content-length             : chr \"360\"\n  ..$ connection                 : chr \"keep-alive\"\n  ..$ access-control-allow-origin: chr \"*\"\n  ..- attr(*, \"class\")= chr [1:2] \"insensitive\" \"list\"\n $ all_headers:List of 1\n  ..$ :List of 3\n  .. ..$ status : int 200\n  .. ..$ version: chr \"HTTP/1.1\"\n  .. ..$ headers:List of 6\n  .. .. ..$ server                     : chr \"nginx/1.10.3\"\n  .. .. ..$ date                       : chr \"Sat, 09 Mar 2024 13:57:20 GMT\"\n  .. .. ..$ content-type               : chr \"application/json\"\n  .. .. ..$ content-length             : chr \"360\"\n  .. .. ..$ connection                 : chr \"keep-alive\"\n  .. .. ..$ access-control-allow-origin: chr \"*\"\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"insensitive\" \"list\"\n $ cookies    :'data.frame':    0 obs. of  7 variables:\n  ..$ domain    : logi(0) \n  ..$ flag      : logi(0) \n  ..$ path      : logi(0) \n  ..$ secure    : logi(0) \n  ..$ expiration: 'POSIXct' num(0) \n  ..$ name      : logi(0) \n  ..$ value     : logi(0) \n $ content    : raw [1:360] 7b 22 6d 65 ...\n $ date       : POSIXct[1:1], format: \"2024-03-09 13:57:20\"\n $ times      : Named num [1:6] 0 0.0367 0.186 0.1861 0.3374 ...\n  ..- attr(*, \"names\")= chr [1:6] \"redirect\" \"namelookup\" \"connect\" \"pretransfer\" ...\n $ request    :List of 7\n  ..$ method    : chr \"GET\"\n  ..$ url       : chr \"http://api.open-notify.org/astros.json\"\n  ..$ headers   : Named chr \"application/json, text/xml, application/xml, */*\"\n  .. ..- attr(*, \"names\")= chr \"Accept\"\n  ..$ fields    : NULL\n  ..$ options   :List of 2\n  .. ..$ useragent: chr \"libcurl/8.1.2 r-curl/5.0.2 httr/1.4.7\"\n  .. ..$ httpget  : logi TRUE\n  ..$ auth_token: NULL\n  ..$ output    : list()\n  .. ..- attr(*, \"class\")= chr [1:2] \"write_memory\" \"write_function\"\n  ..- attr(*, \"class\")= chr \"request\"\n $ handle     :Class 'curl_handle' &lt;externalptr&gt; \n - attr(*, \"class\")= chr \"response\"\n\n\nNous savons désormais que notre objet ovni est une liste comportant 10 branches, elles-mêmes divisées en sous branches qui peuvent être elles-même des listes…\n\n\nRemarque sur les listes\nLes listes sont des objets complexes mais fondamentaux pour la programmation en R. On peut accèder aux branches d’une liste soit en utilisant une série de $ soit en se servant de doubles crochets [[ ]]. Par exemple, si on veut accèder à la date de la réponse on peut taper au choix :\n\novni$headers$date\n\n[1] \"Sat, 09 Mar 2024 13:57:20 GMT\"\n\novni[[\"headers\"]][[\"date\"]]\n\n[1] \"Sat, 09 Mar 2024 13:57:20 GMT\"\n\n\nOn peut également afficher les noms des branches en partant de la racine puis en suivant l’arbre à l’aide de l’instruction names()\n\nnames(ovni$headers)\n\n[1] \"server\"                      \"date\"                       \n[3] \"content-type\"                \"content-length\"             \n[5] \"connection\"                  \"access-control-allow-origin\"\n\n\n\nnames(ovni$fields)\n\nNULL\n\n\n\n\nExtraction des données\nLes données contenues dans la réponse ont été stockées au format JSON (JavaScript Object Notation) qui est devenu un standard pour les échanges de données. Mais elles ont été ensuite comprimées en format binaire pour limiter la taille du fichier transféré. Il va donc falloir procéder en quatre étapes pour les extraire\n\nétape 1 : récupérer les données au format binaire\nOn extrait le champ de données dans la liste. Le résultat est assez étrange :\n\nlibrary(rvest)\ndon_bin&lt;-ovni$content\ndon_bin\n\n  [1] 7b 22 6d 65 73 73 61 67 65 22 3a 20 22 73 75 63 63 65 73 73 22 2c 20 22 70\n [26] 65 6f 70 6c 65 22 3a 20 5b 7b 22 6e 61 6d 65 22 3a 20 22 4a 61 73 6d 69 6e\n [51] 20 4d 6f 67 68 62 65 6c 69 22 2c 20 22 63 72 61 66 74 22 3a 20 22 49 53 53\n [76] 22 7d 2c 20 7b 22 6e 61 6d 65 22 3a 20 22 41 6e 64 72 65 61 73 20 4d 6f 67\n[101] 65 6e 73 65 6e 22 2c 20 22 63 72 61 66 74 22 3a 20 22 49 53 53 22 7d 2c 20\n[126] 7b 22 6e 61 6d 65 22 3a 20 22 53 61 74 6f 73 68 69 20 46 75 72 75 6b 61 77\n[151] 61 22 2c 20 22 63 72 61 66 74 22 3a 20 22 49 53 53 22 7d 2c 20 7b 22 6e 61\n[176] 6d 65 22 3a 20 22 4b 6f 6e 73 74 61 6e 74 69 6e 20 42 6f 72 69 73 6f 76 22\n[201] 2c 20 22 63 72 61 66 74 22 3a 20 22 49 53 53 22 7d 2c 20 7b 22 6e 61 6d 65\n[226] 22 3a 20 22 4f 6c 65 67 20 4b 6f 6e 6f 6e 65 6e 6b 6f 22 2c 20 22 63 72 61\n[251] 66 74 22 3a 20 22 49 53 53 22 7d 2c 20 7b 22 6e 61 6d 65 22 3a 20 22 4e 69\n[276] 6b 6f 6c 61 69 20 43 68 75 62 22 2c 20 22 63 72 61 66 74 22 3a 20 22 49 53\n[301] 53 22 7d 2c 20 7b 22 6e 61 6d 65 22 3a 20 22 4c 6f 72 61 6c 20 4f 27 48 61\n[326] 72 61 22 2c 20 22 63 72 61 66 74 22 3a 20 22 49 53 53 22 7d 5d 2c 20 22 6e\n[351] 75 6d 62 65 72 22 3a 20 37 7d\n\n\n\n\nétape 2 : convertir les données binaires au format caractère\nLa conversion est effectuée à l’aide de la fonction rawToChar() qui fait partie de R-Base.\n\n# conversion du contenu de toto en mode character\ndon_car&lt;-rawToChar(don_bin)\ndon_car\n\n[1] \"{\\\"message\\\": \\\"success\\\", \\\"people\\\": [{\\\"name\\\": \\\"Jasmin Moghbeli\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Andreas Mogensen\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Satoshi Furukawa\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Konstantin Borisov\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Oleg Kononenko\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Nikolai Chub\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Loral O'Hara\\\", \\\"craft\\\": \\\"ISS\\\"}], \\\"number\\\": 7}\"\n\n\nOn commence à mieux voir le résultat mais ce n’est pas encore très lisible car il s’agit de données au format JSON\n\n\nétape 3 : convertir les données JSON en objet R\nOn convertit les données de type JSON en données utilisables par R à l’aide de la fonction fromJson() du package jsonlite()\n\ndon_R &lt;- fromJSON(don_car)\nstr(don_R)\n\nList of 3\n $ message: chr \"success\"\n $ people :'data.frame':    7 obs. of  2 variables:\n  ..$ name : chr [1:7] \"Jasmin Moghbeli\" \"Andreas Mogensen\" \"Satoshi Furukawa\" \"Konstantin Borisov\" ...\n  ..$ craft: chr [1:7] \"ISS\" \"ISS\" \"ISS\" \"ISS\" ...\n $ number : int 7\n\n\nOn obtient finalement une liste de trois éléments dont le dernier est un data.frame décrivant les astronautes présents dans la station spatiale internationale au moment de l’execution du programme.\n\n\nétape 4 : Récupérer le tableau de résultats\n\ntab&lt;-don_R$people\nkable(tab,caption = \"Passagers de l'ISS en temps réel\")\n\n\nPassagers de l’ISS en temps réel\n\n\nname\ncraft\n\n\n\n\nJasmin Moghbeli\nISS\n\n\nAndreas Mogensen\nISS\n\n\nSatoshi Furukawa\nISS\n\n\nKonstantin Borisov\nISS\n\n\nOleg Kononenko\nISS\n\n\nNikolai Chub\nISS\n\n\nLoral O’Hara\nISS\n\n\n\n\n\n\n\n\nEcriture d’une fonction\nUne fois que l’on a bien compris la procédure d’extraction de cette API, on peut construire une fonction d’extraction pour simplifier la tâche et l’automatiser :\n\n## Fonction\nextract_ISS &lt;- function(){\n  ovni &lt;- GET(\"http://api.open-notify.org/astros.json\") \n  don_bin&lt;-ovni$content\n  don_char&lt;-rawToChar(don_bin)\n  don_R&lt;-fromJSON(don_char)\n  tab&lt;-don_R$people\n  return(tab)\n}\n\n## Application\nextract_ISS()\n\n                name craft\n1    Jasmin Moghbeli   ISS\n2   Andreas Mogensen   ISS\n3   Satoshi Furukawa   ISS\n4 Konstantin Borisov   ISS\n5     Oleg Kononenko   ISS\n6       Nikolai Chub   ISS\n7       Loral O'Hara   ISS\n\n\nOn peut améliorer la fonction en lui faisant ajouter un champ qui indique la date à laquelle a été effectué le relevé :\n\n## Fonction\nextract_ISS2 &lt;- function(){\n  ovni &lt;- GET(\"http://api.open-notify.org/astros.json\") \n  don_bin&lt;-ovni$content\n  don_char&lt;-rawToChar(don_bin)\n  don_R&lt;-fromJSON(don_char)\n  tab&lt;-don_R$people\n  tab$date&lt;-ovni$headers$date\n  return(tab)\n}\n\n## Application\nextract_ISS2()\n\n                name craft                          date\n1    Jasmin Moghbeli   ISS Sat, 09 Mar 2024 13:57:21 GMT\n2   Andreas Mogensen   ISS Sat, 09 Mar 2024 13:57:21 GMT\n3   Satoshi Furukawa   ISS Sat, 09 Mar 2024 13:57:21 GMT\n4 Konstantin Borisov   ISS Sat, 09 Mar 2024 13:57:21 GMT\n5     Oleg Kononenko   ISS Sat, 09 Mar 2024 13:57:21 GMT\n6       Nikolai Chub   ISS Sat, 09 Mar 2024 13:57:21 GMT\n7       Loral O'Hara   ISS Sat, 09 Mar 2024 13:57:21 GMT\n\n\nEt si on est à l’aise avec les listes, on peut aussi exporter les résultats sous la forme d’une liste plutôt que d’un tableau, ce qui évite de répéter plusieurs fois la date d’extraction des données\n\n## Fonction\nextract_ISS3 &lt;- function(){\n  ovni &lt;- GET(\"http://api.open-notify.org/astros.json\") \n  don_bin&lt;-ovni$content\n  don_char&lt;-rawToChar(don_bin)\n  don_R&lt;-fromJSON(don_char)\n  tab&lt;-don_R$people\n  date&lt;-ovni$headers$date\n  result&lt;-list(\"Update\" = date,\"Data\" =tab)\n  return(result)\n}\n\n## Application\nx&lt;-extract_ISS3()\nkable(x$Data, caption=paste(\"Passagers de l'ISS :\", x$Update))\n\n\nPassagers de l’ISS : Sat, 09 Mar 2024 13:57:21 GMT\n\n\nname\ncraft\n\n\n\n\nJasmin Moghbeli\nISS\n\n\nAndreas Mogensen\nISS\n\n\nSatoshi Furukawa\nISS\n\n\nKonstantin Borisov\nISS\n\n\nOleg Kononenko\nISS\n\n\nNikolai Chub\nISS\n\n\nLoral O’Hara\nISS\n\n\n\n\n\n\n\nAPI et mise à jour en temps réel\nSur le site web du billet proposé par C. Pascual en février 2020, on trouve une autre liste ne comportant que 6 passagers et avec des noms totalement différents :\n\n\n\nPassagers de l’ISS en février 2020\n\n\ncraft\nname\n\n\n\n\nISS\nChristina Koch\n\n\nISS\nAlexander Skvortsov\n\n\nISS\nLuca Parmitano\n\n\nISS\nAndrew Morgan\n\n\nISS\nOleg Skripochka\n\n\nISS\nJessica Meir\n\n\n\n\n\nEn effet, l’API renvoie les résultats au moment de l’execution de la fonction GET() ce qui correspond à février 2020 pour le billet de blog. Or, les astronautes sont remplacés au plus tous les six mois ce qui explique que tous les noms soient différents un an après.\nNB : Cet exemple permet de mettre en évidence une fonction centrale des API qui est la mise à jour en temps réel des données !"
  },
  {
    "objectID": "11-API-OPENDATASOFT.html",
    "href": "11-API-OPENDATASOFT.html",
    "title": "Pratique des API",
    "section": "",
    "text": "Le but de ce chapitre n’est pas d’apprendre en détail l’ensemble des possibilités qu’offrent les API pour des utilisateurs avancés, mais de fournir aux étudiants en data mining un certain nombre de solutions simples (mais efficaces) pour extraire des données de façon interactive et assurer leur mise à jour régulière.\nOn charge les packages utiles :\nknitr::opts_chunk$set(echo = TRUE, warning= FALSE, message = FALSE, error=FALSE)\n\n## Affichage de tableaux\nlibrary(knitr)\n\n## Requêtes web\nlibrary(httr)\nlibrary(jsonlite)\n\n## Tidyverse & co\nlibrary(dplyr, warn.conflicts = T, quietly = T)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "11-API-OPENDATASOFT.html#choix-dune-api",
    "href": "11-API-OPENDATASOFT.html#choix-dune-api",
    "title": "Pratique des API",
    "section": "Choix d’une API",
    "text": "Choix d’une API\nLa première étape consiste à choisir l’API qui nous intéresse parmi plus de 600.\n\nLe site public.opendatasoft\nNous allons centrer notre chapitre sur le site public.opendatasoft qui permet d’accèder à des centaines d’API à l’aide de requêtes normalisées. Sans apprendre en détail le fonctionnement de cette API, on va montrer comment créer de petites fonctions facilitant le travail d’exportation des variables ou des données.\nOn peut se rendre sur le site pour parcourir les API proposées en allant à l’adresse : https://public.opendatasoft.com\n\n\n\n\n\n\n\nCatalogue des API\nPlutôt que de pacourir le site web, on peut télécharger le catalogue général des bases de données du site public.opendatasoft … en se servant d’une requête API\n\nx&lt;-GET('https://public.opendatasoft.com/api/datasets/1.0/search/?q=&rows=1000&start=0')\ny&lt;-fromJSON(rawToChar((x$content)))\ncat&lt;-y$datasets$metas\nrow.names(cat)&lt;-y$datasets$datasetid\nkable(head(cat[,c(12,1,6,7,8)]),row.names = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\nkeyword\ndomain\nmodified\nlicense\npublisher\n\n\n\n\ngeoref-germany-gemeinde\nGermany , BKG , Gemeinde , Verwaltungsgebiete\npublic\n2020-09-11T16:23:45+00:00\nData licence Germany – attribution – version 2.0\nOpendatasoft\n\n\ngeoref-sweden-kommun-millesime\nkommun , Lantmateriet\npublic\n2022-04-28T00:02:08+00:00\nNA\nOpendatasoft\n\n\nlogpersprestacom\nAllocation , Aide au logement, Logement , APL , ALF , ALS\npublic\n2015-12-11T14:08:02+00:00\nOpen License v1.0\nCAF\n\n\nregistre-parcellaire-graphique-2016\nPAC , Agriculture, Parcellaire, Ilots\npublic\n2018-10-03T17:49:09+00:00\nOpen License v1.0\nAgence de services et de paiement (ASP)\n\n\nejcom\nAllocation , prestations, AF , CF , ASF , AEEH , ARS\npublic\n2015-12-11T11:07:32+00:00\nOpen License v1.0\nCAF\n\n\neconomicref-france-nomenclature-actes-budgetaires-nature-comptes-millesime\nnomenclature des actes budgétaires, collectivités locales , comptabilité , plan de comptes , Totem\npublic\n2021-06-09T11:27:44+00:00\nOpen License v2.0\nDirection Générale des Collectivités Locales (DGCL), DGFIP\n\n\n\n\n\nOn a donc récupéré un tableau qui comporte 605 lignes correspondant à 605 bases de données. Le nom des lignes du tableau indique le code de la base de données que l’on va utiliser ensuite dans les requêtes.\n\n\nChoix d’un tableau de données\nOn suppose que le choix s’est porté sur la base de données dont le nom de code est prix-des-carburants-j-1\n\n\n\n\n\nL’onglet information nous indique qu’il s’agit d’un site produit par le minstère de l’économie et des finances pour faciliter l’accès en temps réel au prix des carburants dans les stations services. Le but est d’informer les conosmmateurs des stations les moins chères à proximité de son domicile afin de stimuler la concurrence et faire baisser les prix.\nIl est indiqué que la base se limite aux prix des douze derniers mois mais nous avons pu vérifier qu’on trouve en fait des données sur plus de trois ans.\n\n\nListe des variables\nAvant de télécharger les données, on regarde précisément la liste des variables disponibles. On peut le faire sur le site web en parcourant les onglets. Mais il est également possible de lancer une requête pour connaître les variables du tableau que l’on va télécharger ainsi que les variables pouvant servir de “facettes” c’est-à-dire permettant d’effectuer des requêtes.\n\ntab&lt;-\"prix-des-carburants-j-1\"\nurl&lt;-paste(\"https://public.opendatasoft.com/api/v2/catalog/datasets/\",tab,\"?\",sep=\"\")\nx&lt;-GET(url)\ny&lt;-fromJSON(rawToChar(x$content))\nvar&lt;-y$dataset$fields\n\nhead(var)\n\n            name description annotations.facet annotations.multivalued\n1             id          NA                NA                    &lt;NA&gt;\n2             cp          NA              TRUE                    &lt;NA&gt;\n3            pop          NA              TRUE                    &lt;NA&gt;\n4        address          NA                NA                    &lt;NA&gt;\n5   com_arm_name          NA              TRUE                    &lt;NA&gt;\n6 automate_24_24          NA              TRUE                    &lt;NA&gt;\n  annotations.facetsort annotations.timeserie_precision annotations.unit\n1                  &lt;NA&gt;                            &lt;NA&gt;             &lt;NA&gt;\n2                  &lt;NA&gt;                            &lt;NA&gt;             &lt;NA&gt;\n3                  &lt;NA&gt;                            &lt;NA&gt;             &lt;NA&gt;\n4                  &lt;NA&gt;                            &lt;NA&gt;             &lt;NA&gt;\n5                  &lt;NA&gt;                            &lt;NA&gt;             &lt;NA&gt;\n6                  &lt;NA&gt;                            &lt;NA&gt;             &lt;NA&gt;\n  annotations.decimals\n1                   NA\n2                   NA\n3                   NA\n4                   NA\n5                   NA\n6                   NA\n                                                      label type\n1                                               Identifiant text\n2                                               Code Postal text\n3                                                  Présence text\n4                                                   Adresse text\n5 Nom Officiel Commune / Arrondissement Municipal Majuscule text\n6                                            Automate 24-24 text\n\n\nOn extrait du tableau les colonnes qui fournissent le nom des variables, leur définition et leur type\n\nvar &lt;- var  %&gt;% select(name, label, type)\nkable(var)\n\n\n\n\n\n\n\n\n\nname\nlabel\ntype\n\n\n\n\nid\nIdentifiant\ntext\n\n\ncp\nCode Postal\ntext\n\n\npop\nPrésence\ntext\n\n\naddress\nAdresse\ntext\n\n\ncom_arm_name\nNom Officiel Commune / Arrondissement Municipal Majuscule\ntext\n\n\nautomate_24_24\nAutomate 24-24\ntext\n\n\ntimetable\nTimetable\ntext\n\n\nfuel\nCarburant\ntext\n\n\nshortage\nRupture\ntext\n\n\nupdate\nMise à jour\ndatetime\n\n\nprice_gazole\nPrix Gazole\ndouble\n\n\nprice_sp95\nPrix SP95\ndouble\n\n\nprice_sp98\nPrix SP98\ndouble\n\n\nprice_gplc\nPrix GPLc\ndouble\n\n\nprice_e10\nPrix E10\ndouble\n\n\nprice_e85\nPrix E85\ndouble\n\n\nservices\nServices\ntext\n\n\nbrand\nMarque\ntext\n\n\nname\nNom\ntext\n\n\ngeo_point\nGeo Point\ngeo_point_2d\n\n\ncom_arm_code\nCode officiel commune ou arrondissement\ntext\n\n\nepci_code\nCode Officiel EPCI\ntext\n\n\nepci_name\nNom Officiel EPCI\ntext\n\n\ndep_code\nCode Officiel Département\ntext\n\n\ndep_name\nNom Officiel Département\ntext\n\n\nreg_code\nCode Officiel Région\ntext\n\n\nreg_name\nNom Officiel Région\ntext\n\n\n\n\n\nOn peut transformer le programme que l’on vient d’executer en fonction pour un usage plus simple :\n\nget_variables&lt;-function(idtab = \"prix-des-carburants-j-1\") {\n  url&lt;-paste(\"https://public.opendatasoft.com/api/v2/catalog/datasets/\",idtab,\"?\",sep=\"\")\n  x&lt;-GET(url)\n  y&lt;-fromJSON(rawToChar((x$content)))\n  var&lt;-y$dataset$fields\n  var &lt;- var %&gt;% select(name, label, type)\n  return(var)\n}\n\nOn peut désormais appliquer notre fonction sur n’importe quel autre tableau du catalogue. Par exemple, si on choisit le tableau qualite_de-lair-france on obtient la liste de variables suivante :\n\nvar&lt;-get_variables(\"qualite-de-lair-france\")\nkable(var)\n\n\n\n\nname\nlabel\ntype\n\n\n\n\ncountry\nCountry Code\ntext\n\n\ncity\nCity\ntext\n\n\nlocation\nLocation\ntext\n\n\ncoordinates\nCoordinates\ngeo_point_2d\n\n\nmeasurements_parameter\nPollutant\ntext\n\n\nmeasurements_sourcename\nSource Name\ntext\n\n\nmeasurements_unit\nUnit\ntext\n\n\nmeasurements_value\nValue\ndouble\n\n\nmeasurements_lastupdated\nLast Updated\ndatetime\n\n\ncountry_name_en\nCountry Label\ntext"
  },
  {
    "objectID": "11-API-OPENDATASOFT.html#récupération-des-données",
    "href": "11-API-OPENDATASOFT.html#récupération-des-données",
    "title": "Pratique des API",
    "section": "Récupération des données",
    "text": "Récupération des données\nPour des utilisateurs non spécialiste, il est difficile de lancer une requête complexe qui suppose une maîtrise avancée des API et des protocoles de requête SOAP et REST. Nous allons opter ici pour une stratégie pragmatique (mais efficace) qui consiste à :\n\nUtiliser l’interface public.opendatasoft pour rédiger une requête\nRécupérer le lien de téléchargement\nTélécharger les données correspondant à la requête\nEffectuer les opérations de nettoyage des données et réaliser un graphique\nModifier le lien et effectuer à nouveau le étapes 3 et 4\nConstruire une fonction paramétrique de téléchargement + nettoyage + visualisation …\n\nPour illustrer cette stratégie, nous allons essayer de créer dans R une fonction automatisée qui télécharge le prix du carburant d’une commune et produit un graphique montrant son évolution au cours du temps dans les dfférentes stations. Nous allons ainsi essayer de reconstituer une application assez proche de celle du ministère de l’économie intitulée “essence pas cher”.\n\n\n\nEssence pas cher\n\n\nNous ne chercherons toutefois pas à obtenir uniquement le dernier prix en date des stations mais plutôt à voir lesquelles sont les plus ou les mons chers sur une période de quelques années.\n\n1. Rédaction d’une requête sur public opendatasoft\nOn utilise les filtres de l’interface pour sélectionner la commune cible à l’aide de son code postal (ex. 94370 = Sucy-en-Brie) et du type carburant (ex. Gazole) :\n\n\n\nFiltres\n\n\n\n\n2. Récupération du lien de téléchargement\nUne fois terminée la mise en place des filtres, on se déplace vers la fenêtre “Export” et on choisit le type de format de sortie que l’on souhaite obtenir. Nous pourrions obtenir des fichiers au format texte (.csv) ou tableur (.xls) mais nous allons adopter ici le format .json qui est plus universel dans le domaine de la data science et qui simplifie les transferts de données entre utilisateurs de différents langages de programmation tels que R ou Python.\nUn click de souris sur le lien nous permet de récupérer l’URL de téléchargement :\n\n\n\nURL\n\n\nMême si certains caractères spéciaux sont difficiles à comprendre comme %3A ou %22 on devine assez facilement la fonction des différents segments de la chaine de caractère qui constitue l’URL de requête :\n\nadresse du site web opendatasoft : https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/\nchoix de la base de données : prix-des-carburants-j-1\nformat d’export et langue : exports/json?lang=fr\nselection du carburant : &refine=fuel%3A%22Gazole%22\nselection de la commune par son code postal : &qv1=(94370)\nfuseau horaire (pour dater la requête) : &timezone=Europe%2FParis\n\n\n\n3. Recupération des données à partir de l’URL\nNous pouvons maintenant rédiger un petit programme très simple qui va récupérer les données à partir de ce lien\n\nlink&lt;-\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(94370)&timezone=Europe%2FParis\"\ny&lt;-fromJSON(link)\nhead(y)\n\n        id    cp pop                     address com_arm_name automate_24_24\n1 94370008 94370   R 63/71 AV DU GENERAL LECLERC SUCY-EN-BRIE            Oui\n2 94370007 94370   R     13 Rue Maurice Berteaux SUCY-EN-BRIE            Non\n3 94370008 94370   R 63/71 AV DU GENERAL LECLERC SUCY-EN-BRIE            Oui\n4 94370003 94370   R              1 Rue de Paris SUCY-EN-BRIE            Oui\n5 94370003 94370   R              1 Rue de Paris SUCY-EN-BRIE            Oui\n6 94370008 94370   R 63/71 AV DU GENERAL LECLERC SUCY-EN-BRIE            Oui\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               timetable\n1 {\"Dimanche\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"08.00\"}, \"Jeudi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Lundi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Mardi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Mercredi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Samedi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"08.00\"}, \"Vendredi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}}\n2 {\"Dimanche\": {\"fermeture\": \"22.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Jeudi\": {\"fermeture\": \"23.00\", \"ouvert\": 1, \"ouverture\": \"06.00\"}, \"Lundi\": {\"fermeture\": \"23.00\", \"ouvert\": 1, \"ouverture\": \"06.00\"}, \"Mardi\": {\"fermeture\": \"23.00\", \"ouvert\": 1, \"ouverture\": \"06.00\"}, \"Mercredi\": {\"fermeture\": \"23.00\", \"ouvert\": 1, \"ouverture\": \"06.00\"}, \"Samedi\": {\"fermeture\": \"23.00\", \"ouvert\": 1, \"ouverture\": \"06.00\"}, \"Vendredi\": {\"fermeture\": \"23.00\", \"ouvert\": 1, \"ouverture\": \"06.00\"}}\n3 {\"Dimanche\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"08.00\"}, \"Jeudi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Lundi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Mardi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Mercredi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Samedi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"08.00\"}, \"Vendredi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}}\n4                                                                                                                                                                                                                                                                                                                     {\"Dimanche\": {\"ouvert\": 1}, \"Jeudi\": {\"ouvert\": 1}, \"Lundi\": {\"ouvert\": 1}, \"Mardi\": {\"ouvert\": 1}, \"Mercredi\": {\"ouvert\": 1}, \"Samedi\": {\"ouvert\": 1}, \"Vendredi\": {\"ouvert\": 1}}\n5                                                                                                                                                                                                                                                                                                                     {\"Dimanche\": {\"ouvert\": 1}, \"Jeudi\": {\"ouvert\": 1}, \"Lundi\": {\"ouvert\": 1}, \"Mardi\": {\"ouvert\": 1}, \"Mercredi\": {\"ouvert\": 1}, \"Samedi\": {\"ouvert\": 1}, \"Vendredi\": {\"ouvert\": 1}}\n6 {\"Dimanche\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"08.00\"}, \"Jeudi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Lundi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Mardi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Mercredi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}, \"Samedi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"08.00\"}, \"Vendredi\": {\"fermeture\": \"21.00\", \"ouvert\": 1, \"ouverture\": \"07.00\"}}\n               fuel                   shortage                    update\n1            Gazole E85, SP95, GPLc, E10, SP98 2022-10-24T00:01:00+02:00\n2 Gazole, E10, SP98      SP95, E85, SP95, GPLc 2022-11-08T00:01:00+01:00\n3 Gazole, E10, SP98            SP95, E85, GPLc 2022-12-06T00:01:00+01:00\n4 Gazole, E10, SP98                       SP95 2022-12-21T17:40:00+01:00\n5 Gazole, E10, SP98                       SP95 2023-01-07T02:46:00+01:00\n6 Gazole, E10, SP98            SP95, E85, GPLc 2023-01-18T00:01:00+01:00\n  price_gazole price_sp95 price_sp98 price_gplc price_e10 price_e85\n1     0.001818         NA         NA         NA        NA        NA\n2     0.001847         NA   0.001679         NA  0.001569        NA\n3     0.001833         NA   0.001759         NA  0.001649        NA\n4     0.001778         NA   0.001691         NA  0.001611        NA\n5     0.001915         NA   0.001903         NA  0.001823        NA\n6     0.001961         NA   0.001970         NA  0.001882        NA\n                                                                                                                                                                                                                              services\n1                     Boutique alimentaire, Boutique non alimentaire, Station de gonflage, Carburant additivé, Lavage automatique, Lavage manuel, Vente de gaz domestique (Butane, Propane), DAB (Distributeur automatique de billets)\n2 Boutique alimentaire, Boutique non alimentaire, Restauration à emporter, Vente de fioul domestique, Station de gonflage, Carburant additivé, Lavage automatique, Vente de gaz domestique (Butane, Propane), Wifi, Automate CB 24, 24\n3                     Boutique alimentaire, Boutique non alimentaire, Station de gonflage, Carburant additivé, Lavage automatique, Lavage manuel, Vente de gaz domestique (Butane, Propane), DAB (Distributeur automatique de billets)\n4                                                                                                                                                                                                                                 NULL\n5                                                                                                                                                                                                                                 NULL\n6                     Boutique alimentaire, Boutique non alimentaire, Station de gonflage, Carburant additivé, Lavage automatique, Lavage manuel, Vente de gaz domestique (Butane, Propane), DAB (Distributeur automatique de billets)\n         brand              name geo_point.lon geo_point.lat com_arm_code\n1         &lt;NA&gt;              &lt;NA&gt;            NA            NA         &lt;NA&gt;\n2        Total       SARL DURMUS       2.51743      48.77333        94071\n3         &lt;NA&gt;              &lt;NA&gt;            NA            NA         &lt;NA&gt;\n4 Esso Express ESSO PETIT MARAIS       2.49956      48.77306        94071\n5 Esso Express ESSO PETIT MARAIS       2.49956      48.77306        94071\n6         &lt;NA&gt;              &lt;NA&gt;            NA            NA         &lt;NA&gt;\n  epci_code                epci_name dep_code     dep_name reg_code\n1      &lt;NA&gt;                     &lt;NA&gt;     &lt;NA&gt;         &lt;NA&gt;     &lt;NA&gt;\n2 200054781 Métropole du Grand Paris       94 Val-de-Marne       11\n3      &lt;NA&gt;                     &lt;NA&gt;     &lt;NA&gt;         &lt;NA&gt;     &lt;NA&gt;\n4 200054781 Métropole du Grand Paris       94 Val-de-Marne       11\n5 200054781 Métropole du Grand Paris       94 Val-de-Marne       11\n6      &lt;NA&gt;                     &lt;NA&gt;     &lt;NA&gt;         &lt;NA&gt;     &lt;NA&gt;\n       reg_name\n1          &lt;NA&gt;\n2 Île-de-France\n3          &lt;NA&gt;\n4 Île-de-France\n5 Île-de-France\n6          &lt;NA&gt;\n\n\nA la différence de la méthode GET vue au chapitre précédent, nous récupérons directement le fichier de données sans avoir besoin d’effectuer des transformations de type RawToChar. C’est donc beaucoup plus simple mais, en contrepartie, nous perdons toute une série d’informations qu’apportait la procédure dans les règles de l’art (date de téléchargement, messages d’erreur, version des données, etc.).\n\n\n4. Nettoyage des données\nNous procédons ensuite à un petit nettoyage pour ne garder que les variables utiles :\n\nnames(y)\n\n [1] \"id\"             \"cp\"             \"pop\"            \"address\"       \n [5] \"com_arm_name\"   \"automate_24_24\" \"timetable\"      \"fuel\"          \n [9] \"shortage\"       \"update\"         \"price_gazole\"   \"price_sp95\"    \n[13] \"price_sp98\"     \"price_gplc\"     \"price_e10\"      \"price_e85\"     \n[17] \"services\"       \"brand\"          \"name\"           \"geo_point\"     \n[21] \"com_arm_code\"   \"epci_code\"      \"epci_name\"      \"dep_code\"      \n[25] \"dep_name\"       \"reg_code\"       \"reg_name\"      \n\ndon &lt;- y %&gt;% select(name,address, update, price = price_gazole ) %&gt;% \n  mutate(update =as.Date(update)) %&gt;%\n  arrange(update)\n\nIl y a toutefois une mauvaise surprise … les données semblent erronées à partir d’une certaine date\n\nggplot(don) +aes(x=update, y=price, col=address) + geom_point()\n\n\n\n\nEn fait … les chiffres qui sont fournis après le 26 mars ont été divisés mystérieusement par 1000. Il faut donc corriger ce problème :\n\nlibrary(ggplot2)\ndon&lt;-don %&gt;%  mutate(price_OK = case_when(price ==0 ~ NA,\n                             price &lt; 1 ~ price*1000,\n                             TRUE ~ price))\nggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()\n\n\n\n\nOn note qu’il este une valeur aberrante mais sinon il est désormais possible de bien suivre l’évolution des prix au cours des trois dernières années et de repérer quelles est la station la moins chèr aux différentes dates.\n\n\n5. Changement de lien\nEssayons maintenant de reprendre l’ensemble de notre programme en changeant juste de commune dans le lien initial. On va ici soigner la rédaction du programme car nous comptons ensuite le transformer en fonction\nOn remplace le code postal de Sucy-en-Brie (94370) par celui d’Ivry-sur-Seine(94200).\n\n# Choix du lien (changement du code postal)\nlink&lt;-\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(94200)&timezone=Europe%2FParis\"\n\n# Importation des données\ny&lt;-fromJSON(link)\n\n# Selection des variables\ndon &lt;- y %&gt;% select(name,address, update, price = price_gazole ) %&gt;% \n  mutate(update =as.Date(update)) %&gt;%\n  arrange(update)\n\n# Nettoyage des erreurs principales\ndon&lt;-don %&gt;%  mutate(price_OK = case_when(price ==0 ~ NA,\n                             price &lt; 1 ~ price*1000,\n                             TRUE ~ price))\n\n# Réalisation d'un graphique\nggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()\n\n\n\n\n\n\n6. Rédaction d’une Fonction\nOn peut maintenant écrire une fonction qui ne va dépendre que du code postal et va fournir en sortie le tableau de données. Tout ce que nous avons à faire est de modifier le lien en fonction du code postal qui sera le paramètre de la fonction.\nPour cela nous utilisons la commande R pasteO()qui permet de coller des chaînes de caractères sans ajouter d’expaces. Ici nous recollons le début de l’URL, le code de la commune que nous avons modifié et la fin de l’URL.\n\ngazole_tab &lt;- function(code=\"94370\") { \n# Choix du lien (changement du code postal)\nlink&lt;-paste0(\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(\", code,\")&timezone=Europe%2FParis\")\n\n# Importation des données\ny&lt;-fromJSON(link)\n\n# Selection des variables\ntab &lt;- y %&gt;% select(name,address, update, price = price_gazole ) %&gt;% \n  mutate(update =as.Date(update)) %&gt;%\n  arrange(update)\n\n# Nettoyage des erreurs principales\ntab&lt;-tab %&gt;%  mutate(price_OK = case_when(price ==0 ~ NA,\n                             price &lt; 1 ~ price*1000,\n                             TRUE ~ price))\n\nreturn(tab)\n\n}\n\nPour tester notre fonction gazole_tab(), on prend en exemple une nouvelle commune, par exemple Saint-Maur des Fossés (94100) :\n\nres&lt;-gazole_tab(\"94100\")\nhead(res)\n\n              name            address     update price price_OK\n1             &lt;NA&gt;   57 BD DE CRETEIL 2021-02-18 1.499    1.499\n2 Carrefour Market    57, Rue Delenue 2021-02-18 1.366    1.366\n3             &lt;NA&gt;  29 bvd de créteil 2021-02-18 1.452    1.452\n4        ESSO FOCH 99/101 Avenue Foch 2021-02-18 1.366    1.366\n5 Carrefour Market    57, Rue Delenue 2021-02-19 1.371    1.371\n6        ESSO FOCH 99/101 Avenue Foch 2021-02-19 1.371    1.371\n\n\nMais on pourrait aussi faire une fonction gazole_graph()qui renvoie non pas le tableau mais le graphique :\n\ngazole_graph &lt;- function(code=\"94370\") { \n# Choix du lien (changement du code postal)\nlink&lt;-paste0(\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(\", code,\")&timezone=Europe%2FParis\")\n\n# Importation des données\ny&lt;-fromJSON(link)\n\n# Selection des variables\ndon &lt;- y %&gt;% select(name,address, update, price = price_gazole ) %&gt;% \n  mutate(update =as.Date(update)) %&gt;%\n  arrange(update)\n\n# Nettoyage des erreurs principales\ndon&lt;-don %&gt;%  mutate(price_OK = case_when(price ==0 ~ NA,\n                             price &lt; 1 ~ price*1000,\n                             TRUE ~ price))\n\n# Réalisation d'un graphique\ngraph&lt;-ggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()\n\nreturn(graph)\n\n}\n\nOn teste la fonction sur Saint-Maur des Fossés (94100) :\n\ngazole_graph(\"94100\")\n\n\n\n\nMais le plus intéressant est de faire une fonction unique gazole()qui permet de renvoyer à la fois le tableau et le graphique en indiquant en sortie une liste d’objets comprenant à la fois le tableau (objet de type data.frame) et le graphique (objet de type ggplot2).\n\ngazole &lt;- function(code=\"94370\") { \n# Choix du lien (changement du code postal)\nlink&lt;-paste0(\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(\", code,\")&timezone=Europe%2FParis\")\n\n# Importation des données\ny&lt;-fromJSON(link)\n\n# Selection des variables\ntab &lt;- y %&gt;% select(name,address, update, price = price_gazole ) %&gt;% \n  mutate(update =as.Date(update)) %&gt;%\n  arrange(update)\n\n# Nettoyage des erreurs principales\ntab&lt;-tab %&gt;%  mutate(price_OK = case_when(price ==0 ~ NA,\n                             price &lt; 1 ~ price*1000,\n                             TRUE ~ price))\n\n# Réalisation d'un graphique\ngraph&lt;-ggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()\n\nreturn(list(\"tab\"=tab, \"graph\"=graph))\n\n}\n\nIl suffit maintenant d’executer une seule fois la fonction (un seul appel de l’API) pour pouvoir ensuite au choix utiliser le tableau ou afficher le graphique.\n\nres&lt;-gazole(\"94100\")\nhead(res$tab)\n\n              name            address     update price price_OK\n1             &lt;NA&gt;   57 BD DE CRETEIL 2021-02-18 1.499    1.499\n2 Carrefour Market    57, Rue Delenue 2021-02-18 1.366    1.366\n3             &lt;NA&gt;  29 bvd de créteil 2021-02-18 1.452    1.452\n4        ESSO FOCH 99/101 Avenue Foch 2021-02-18 1.366    1.366\n5 Carrefour Market    57, Rue Delenue 2021-02-19 1.371    1.371\n6        ESSO FOCH 99/101 Avenue Foch 2021-02-19 1.371    1.371\n\nres$graph"
  },
  {
    "objectID": "11-API-OPENDATASOFT.html#conclusion",
    "href": "11-API-OPENDATASOFT.html#conclusion",
    "title": "Pratique des API",
    "section": "Conclusion",
    "text": "Conclusion\nCe chapitre a permis de combiner trois apprentissages fondamentaux du data mining qui seront repris ensuie à plusieurs reprises :\n\nUtiliser des API pour récupérer directement ses données sans effectuer de téléchargement “à la main”.\nNettoyer les données reçues avant de les utiliser et automatiser autant que possible les procédures de nettoyages.\nCréer ses propres fonctions pour automatiser les tâches de récupération des données, nettoyage et production de tableaux ou graphiques."
  }
]